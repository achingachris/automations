name: Daily Scrape

on:
  schedule:
    # Africa/Nairobi (UTC+3) converted to UTC
    - cron: "0 4 * * *"    # 07:00 EAT
    - cron: "0 9 * * *"    # 12:00 EAT
    - cron: "0 12 * * *"   # 15:00 EAT
    - cron: "0 16 * * *"   # 19:00 EAT
    - cron: "59 20 * * *"  # 23:59 EAT
  workflow_dispatch:

permissions:
  contents: write

env:
  TZ: Africa/Nairobi

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: pip install feedparser

      - name: Create daily placeholder files (EAT)
        run: |
          set -e
          # Use TZ environment variable for proper timezone handling
          # Structure: content/{type}/YYYY/MM/DD.md
          YEAR=$(date +'%Y')
          MONTH=$(date +'%m')
          DAY=$(date +'%d')
          DATE_STR=$(date +'%d-%m-%Y')

          # Articles placeholder
          ARTICLES_DIR="content/articles/${YEAR}/${MONTH}"
          mkdir -p "$ARTICLES_DIR"
          ARTICLES_FILE="${ARTICLES_DIR}/${DAY}.md"
          if [ ! -f "$ARTICLES_FILE" ]; then
            {
              echo "# Daily Tech Articles (${DATE_STR})"
              echo
              echo "Summary: 0 articles yet (placeholder created by CI)"
              echo
              echo "| # | date | title | url | summary |"
              echo "| --- | --- | --- | --- | --- |"
            } > "$ARTICLES_FILE"
          fi
          test -s "$ARTICLES_FILE" || (echo "Articles placeholder not created" && exit 1)
          echo "Using articles file: $ARTICLES_FILE"

          # Newsletters placeholder
          NEWSLETTERS_DIR="content/newsletters/${YEAR}/${MONTH}"
          mkdir -p "$NEWSLETTERS_DIR"
          NEWSLETTERS_FILE="${NEWSLETTERS_DIR}/${DAY}.md"
          if [ ! -f "$NEWSLETTERS_FILE" ]; then
            {
              echo "# Daily Newsletters (${DATE_STR})"
              echo
              echo "Summary: 0 newsletters yet (placeholder created by CI)"
              echo
              echo "| # | date | newsletter | title | url |"
              echo "| --- | --- | --- | --- | --- |"
            } > "$NEWSLETTERS_FILE"
          fi
          test -s "$NEWSLETTERS_FILE" || (echo "Newsletters placeholder not created" && exit 1)
          echo "Using newsletters file: $NEWSLETTERS_FILE"

          # Social media placeholder
          SOCIAL_DIR="content/social/${YEAR}/${MONTH}"
          mkdir -p "$SOCIAL_DIR"
          SOCIAL_FILE="${SOCIAL_DIR}/${DAY}.md"
          if [ ! -f "$SOCIAL_FILE" ]; then
            {
              echo "# Daily Social Media (${DATE_STR})"
              echo
              echo "Summary: 0 posts yet (placeholder created by CI)"
              echo
              echo "| # | date | source | content | url |"
              echo "| --- | --- | --- | --- | --- |"
            } > "$SOCIAL_FILE"
          fi
          test -s "$SOCIAL_FILE" || (echo "Social placeholder not created" && exit 1)
          echo "Using social file: $SOCIAL_FILE"

      - name: Run article scraper
        run: python scripts/scrape_daily_articles.py

      - name: Run newsletter scraper
        run: python scripts/scrape_newsletters.py

      - name: Run social media scraper
        run: python scripts/scrape_social.py

      - name: Update README stats
        run: |
          set -e

          YEAR=$(date +'%Y')
          MONTH=$(date +'%m')
          DAY=$(date +'%d')
          DATE_STR=$(date +'%d-%m-%Y')
          LAST_UPDATED=$(date +'%Y-%m-%d %H:%M EAT')

          # Count totals across all content
          TOTAL_ARTICLES=$(find content/articles -name "*.md" -exec grep -h "^|.*http" {} \; 2>/dev/null | wc -l | tr -d ' ')
          TOTAL_NEWSLETTERS=$(find content/newsletters -name "*.md" -exec grep -h "^|.*http" {} \; 2>/dev/null | wc -l | tr -d ' ')
          TOTAL_SOCIAL=$(find content/social -name "*.md" -exec grep -h "^|.*http" {} \; 2>/dev/null | wc -l | tr -d ' ')

          # Count today's entries
          TODAY_ARTICLES=0
          TODAY_NEWSLETTERS=0
          TODAY_SOCIAL=0

          ARTICLES_FILE="content/articles/${YEAR}/${MONTH}/${DAY}.md"
          NEWSLETTERS_FILE="content/newsletters/${YEAR}/${MONTH}/${DAY}.md"
          SOCIAL_FILE="content/social/${YEAR}/${MONTH}/${DAY}.md"

          if [ -f "$ARTICLES_FILE" ]; then
            TODAY_ARTICLES=$(grep -c "^|.*http" "$ARTICLES_FILE" 2>/dev/null || echo 0)
          fi
          if [ -f "$NEWSLETTERS_FILE" ]; then
            TODAY_NEWSLETTERS=$(grep -c "^|.*http" "$NEWSLETTERS_FILE" 2>/dev/null || echo 0)
          fi
          if [ -f "$SOCIAL_FILE" ]; then
            TODAY_SOCIAL=$(grep -c "^|.*http" "$SOCIAL_FILE" 2>/dev/null || echo 0)
          fi

          # Update README stats + daily log (portable, avoids sed/awk quirks)
          export TOTAL_ARTICLES TOTAL_NEWSLETTERS TOTAL_SOCIAL LAST_UPDATED
          export DATE_STR TODAY_ARTICLES TODAY_NEWSLETTERS TODAY_SOCIAL
          python - <<'PY'
          from pathlib import Path
          import os
          import re
          import sys

          readme_path = Path("README.md")
          text = readme_path.read_text()

          stats_block = "\n".join(
              [
                  "<!-- STATS_START -->",
                  "| Metric | Count |",
                  "| --- | --- |",
                  f"| Total Articles | {os.environ['TOTAL_ARTICLES']} |",
                  f"| Total Newsletters | {os.environ['TOTAL_NEWSLETTERS']} |",
                  f"| Total Social Posts | {os.environ['TOTAL_SOCIAL']} |",
                  f"| Last Updated | {os.environ['LAST_UPDATED']} |",
                  "<!-- STATS_END -->",
              ]
          )

          if "<!-- STATS_START -->" not in text or "<!-- STATS_END -->" not in text:
              sys.exit("Stats markers not found in README.md")

          text = re.sub(
              r"<!-- STATS_START -->.*?<!-- STATS_END -->",
              stats_block,
              text,
              flags=re.S,
          )

          date_str = os.environ["DATE_STR"]
          day, month, year = date_str.split("-")
          articles_path = f"content/articles/{year}/{month}/{day}.md"
          newsletters_path = f"content/newsletters/{year}/{month}/{day}.md"
          social_path = f"content/social/{year}/{month}/{day}.md"

          new_row = (
              f"| {date_str} | [{os.environ['TODAY_ARTICLES']}]({articles_path}) | "
              f"[{os.environ['TODAY_NEWSLETTERS']}]({newsletters_path}) | "
              f"[{os.environ['TODAY_SOCIAL']}]({social_path}) |"
          )

          m = re.search(
              r"<!-- DAILY_LOG_START -->\n(.*?)\n<!-- DAILY_LOG_END -->",
              text,
              flags=re.S,
          )
          if not m:
              sys.exit("Daily log markers not found in README.md")

          block = m.group(1).strip("\n")
          lines = block.splitlines()
          if len(lines) < 2:
              sys.exit("Daily log header missing in README.md")

          header = [
              "| Date | Articles | Newsletters | Social |",
              "| --- | --- | --- | --- |",
          ]
          data_lines = [line for line in lines[2:] if line.strip()]

          updated = False
          for i, line in enumerate(data_lines):
              if line.startswith(f"| {os.environ['DATE_STR']} |"):
                  data_lines[i] = new_row
                  updated = True
                  break

          if not updated:
              data_lines.insert(0, new_row)

          data_lines = data_lines[:10]
          new_block = "\n".join(header + data_lines)
          text = text[: m.start(1)] + new_block + text[m.end(1) :]

          if not text.endswith("\n"):
              text += "\n"

          readme_path.write_text(text)
          PY

          echo "Updated README: $TOTAL_ARTICLES articles, $TOTAL_NEWSLETTERS newsletters, $TOTAL_SOCIAL social posts"
          echo "Today (${DATE_STR}): $TODAY_ARTICLES articles, $TODAY_NEWSLETTERS newsletters, $TODAY_SOCIAL social posts"

      - name: Pull latest changes before commit
        run: |
          git pull --rebase origin ${{ github.ref_name }} || true

      - name: Commit and push changes
        run: |
          set -e

          git config user.name "C A - github bot"
          git config user.email "achinga.chris@gmail.com"

          # Stage all content directories
          git add content/ content-source/ README.md

          # Check staged changes (includes new files)
          if git diff --cached --quiet; then
            echo "No changes to commit"
            exit 0
          fi

          git commit -m "chore: update daily articles, newsletters, and social posts"

          # Retry push with exponential backoff on failure
          for i in 1 2 3 4; do
            if git push; then
              echo "Push successful"
              exit 0
            fi
            echo "Push failed, retrying in $((2**i)) seconds..."
            sleep $((2**i))
          done
          echo "Push failed after 4 retries"
          exit 1
