name: Daily Scrape

on:
  schedule:
    # Africa/Nairobi (UTC+3) converted to UTC
    - cron: "0 4 * * *"    # 07:00 EAT
    - cron: "0 9 * * *"    # 12:00 EAT
    - cron: "0 12 * * *"   # 15:00 EAT
    - cron: "0 16 * * *"   # 19:00 EAT
    - cron: "59 20 * * *"  # 23:59 EAT
  workflow_dispatch:

permissions:
  contents: write

env:
  TZ: Africa/Nairobi

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: pip install feedparser

      - name: Create daily placeholder files (EAT)
        run: |
          set -e
          # Use TZ environment variable for proper timezone handling
          DATE_EAT=$(date +'%d-%m-%Y')

          # Articles placeholder
          ARTICLES_FILE="daily-articles/${DATE_EAT}.md"
          if [ ! -f "$ARTICLES_FILE" ]; then
            {
              echo "# Daily Tech Articles (${DATE_EAT})"
              echo
              echo "Summary: 0 articles yet (placeholder created by CI)"
              echo
              echo "| # | date | title | url | summary |"
              echo "| --- | --- | --- | --- | --- |"
            } > "$ARTICLES_FILE"
          fi
          test -s "$ARTICLES_FILE" || (echo "Articles placeholder not created" && exit 1)
          echo "Using articles file: $ARTICLES_FILE"

          # Newsletters placeholder
          mkdir -p daily-newsletters
          NEWSLETTERS_FILE="daily-newsletters/${DATE_EAT}.md"
          if [ ! -f "$NEWSLETTERS_FILE" ]; then
            {
              echo "# Daily Newsletters (${DATE_EAT})"
              echo
              echo "Summary: 0 newsletters yet (placeholder created by CI)"
              echo
              echo "| # | date | newsletter | title | url |"
              echo "| --- | --- | --- | --- | --- |"
            } > "$NEWSLETTERS_FILE"
          fi
          test -s "$NEWSLETTERS_FILE" || (echo "Newsletters placeholder not created" && exit 1)
          echo "Using newsletters file: $NEWSLETTERS_FILE"

      - name: Run article scraper
        run: python scripts/scrape_daily_articles.py

      - name: Run newsletter scraper
        run: python scripts/scrape_newsletters.py

      - name: Update README stats
        run: |
          set -e

          DATE_EAT=$(date +'%d-%m-%Y')
          LAST_UPDATED=$(date +'%Y-%m-%d %H:%M EAT')

          # Count total articles and newsletters
          TOTAL_ARTICLES=$(grep -rh "^|.*http" daily-articles/*.md 2>/dev/null | wc -l | tr -d ' ')
          TOTAL_NEWSLETTERS=$(grep -rh "^|.*http" daily-newsletters/*.md 2>/dev/null | wc -l | tr -d ' ')

          # Count today's articles and newsletters
          TODAY_ARTICLES=0
          TODAY_NEWSLETTERS=0
          if [ -f "daily-articles/${DATE_EAT}.md" ]; then
            TODAY_ARTICLES=$(grep -c "^|.*http" "daily-articles/${DATE_EAT}.md" 2>/dev/null || echo 0)
          fi
          if [ -f "daily-newsletters/${DATE_EAT}.md" ]; then
            TODAY_NEWSLETTERS=$(grep -c "^|.*http" "daily-newsletters/${DATE_EAT}.md" 2>/dev/null || echo 0)
          fi

          # Update total stats section
          sed -i '/<!-- STATS_START -->/,/<!-- STATS_END -->/c\<!-- STATS_START -->\n| Metric | Count |\n| --- | --- |\n| Total Articles | '"$TOTAL_ARTICLES"' |\n| Total Newsletters | '"$TOTAL_NEWSLETTERS"' |\n| Last Updated | '"$LAST_UPDATED"' |\n<!-- STATS_END -->' README.md

          # Update daily log (keep last 10 entries)
          NEW_ROW="| ${DATE_EAT} | ${TODAY_ARTICLES} | ${TODAY_NEWSLETTERS} |"

          # Check if today's entry already exists, if so update it, otherwise add new row
          if grep -q "| ${DATE_EAT} |" README.md; then
            sed -i "s/| ${DATE_EAT} |.*|.*|/${NEW_ROW}/" README.md
          else
            # Add new row after header, keep only last 10 entries
            sed -i '/<!-- DAILY_LOG_START -->/,/<!-- DAILY_LOG_END -->/{
              /| --- | --- | --- |/a\'"${NEW_ROW}"'
            }' README.md

            # Keep only last 10 data rows (plus header)
            awk '
              /<!-- DAILY_LOG_START -->/{p=1; print; next}
              /<!-- DAILY_LOG_END -->/{p=0; for(i=1;i<=2;i++) print header[i]; for(i=(c>10?c-9:1);i<=c;i++) print lines[i]; print; next}
              p && /^\| ---/{header[1]=prev; header[2]=$0; next}
              p && /^\|/{lines[++c]=$0}
              p{prev=$0; next}
              !p{print}
            ' README.md > README.tmp && mv README.tmp README.md
          fi

          echo "Updated README: $TOTAL_ARTICLES total articles, $TOTAL_NEWSLETTERS total newsletters"
          echo "Today (${DATE_EAT}): $TODAY_ARTICLES articles, $TODAY_NEWSLETTERS newsletters"

      - name: Pull latest changes before commit
        run: |
          git pull --rebase origin ${{ github.ref_name }} || true

      - name: Commit and push changes
        run: |
          set -e

          git config user.name "C A - github bot"
          git config user.email "achinga.chris@gmail.com"

          # Stage first so new files are included
          git add daily-articles daily-newsletters content-source README.md

          # Check staged changes (includes new files)
          if git diff --cached --quiet; then
            echo "No changes to commit"
            exit 0
          fi

          git commit -m "chore: update daily articles and newsletters"

          # Retry push with exponential backoff on failure
          for i in 1 2 3 4; do
            if git push; then
              echo "Push successful"
              exit 0
            fi
            echo "Push failed, retrying in $((2**i)) seconds..."
            sleep $((2**i))
          done
          echo "Push failed after 4 retries"
          exit 1
