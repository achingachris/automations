name: Daily Article Scrape

on:
  schedule:
    # Africa/Nairobi (UTC+3) converted to UTC
    - cron: "0 4 * * *"    # 07:00 EAT
    - cron: "0 9 * * *"    # 12:00 EAT
    - cron: "0 12 * * *"   # 15:00 EAT
    - cron: "0 16 * * *"   # 19:00 EAT
    - cron: "59 20 * * *"  # 23:59 EAT
  workflow_dispatch:

permissions:
  contents: write

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: pip install feedparser

      - name: Create daily placeholder file (EAT)
        run: |
          set -e
          DATE_EAT=$(date -u -d "+3 hours" +'%d-%m-%Y')
          FILE="daily-articles/${DATE_EAT}.md"

          if [ ! -f "$FILE" ]; then
            {
              echo "# Daily Tech Articles (${DATE_EAT})"
              echo
              echo "Summary: 0 articles yet (placeholder created by CI)"
              echo
              echo "| # | date | title/topic | url | tag | summary |"
              echo "| --- | --- | --- | --- | --- | --- |"
            } > "$FILE"
          fi

          test -s "$FILE" || (echo "Placeholder file not created" && exit 1)
          echo "Using file: $FILE"

      - name: Run scraper
        run: python scripts/scrape_daily_articles.py

      - name: Commit and push changes
        run: |
          set -e

          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          # Stage first so new files are included
          git add daily-articles content-source

          # Check staged changes (includes new files)
          if git diff --cached --quiet; then
            echo "No changes to commit"
            exit 0
          fi

          git commit -m "chore: update daily articles"
          git push
